{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as torch\n",
    "from torch import nn\n",
    "\n",
    "class InputEmbeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    # creates an input embedding \"lookup table\" \n",
    "    # x: 1d tensor representing input tokens ->\n",
    "    # returns [vocab_size, d_model] tensor \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * torch.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, dropout):\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # dropout prevents co-adapation of neurons/feature-detectors \n",
    "        # probability that any neuron's output will be \"dropped\" i.e. val set to 0\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # create a 2d tensor (seq_len, d_model)\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # 1d vector of (seq_len, 1) representing position of word in sentence\n",
    "        pos = torch.arrange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # term to multiply by pos and run through sin/cos value for pe(n, i)\n",
    "        div_term = torch.exp(torch.arrange(0, d_model, 2).float()*(-torch.log(10000.0)/d_model))\n",
    "        \n",
    "        # apply positional values\n",
    "        pe[:,0::2] = torch.sin(pos * div_term)\n",
    "        pe[:,1::2] = torch.cos(pos * div_term)\n",
    "        \n",
    "        # \"batchify\" from [seq_len,d_model] -> [1,seq_len,d_model]\n",
    "        pe.unsqueeze(0)\n",
    "        \n",
    "        # saves tensor as a file\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + (self.pe[:, :x.shape[1], :])\n",
    "        \n",
    "        # makes this tensor \"unlearned\", doesn't perform gradient descent\n",
    "        x = x.requires_grad_(False)\n",
    "        \n",
    "        # pass through dropout for regularization \n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, eps=(10**-6)):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "        # after normalization, each element of the input vector is\n",
    "        # scaled by an alpha and has its own bias which are learnable \n",
    "        # parameters\n",
    "        self.alpha = nn.Parameter(torch.ones(1))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=-1, keepdim=True)\n",
    "        std = torch.mean(x, dim=-1, keepdim=True)\n",
    "        \n",
    "        return (\n",
    "            self.alpha * \n",
    "            (x - mean)/(std + self.eps)\n",
    "            + self.bias\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, dropout, d_model=512, d_ff=2048):\n",
    "        super.__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    # x: (batch, seq_len, d_model)\n",
    "    # -> (batch, seq_len, d_ff) layer 1 \n",
    "    # -> (batch, seq_len, d_model) layer 2\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # # first layer\n",
    "        # x = torch.max(0, self.linear_1(x))\n",
    "        \n",
    "        # # relu layer \n",
    "        # x = self.relu(x)a\n",
    "        \n",
    "        # # second linear layer\n",
    "        # x = self.linear_2(x)\n",
    "        \n",
    "        # # dropout for regularization\n",
    "        # return self.dropout(x)\n",
    "        \n",
    "        return self.linear_2(self.dropout(self.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout):\n",
    "        super.__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # number of heads that Q, K, and V will be split into\n",
    "        # heads are split so each head contains the entire input sequence\n",
    "        self.h = h\n",
    "        assert d_model % h == 0, \"d_model needs to be divisible by h\"\n",
    "        self.dk = d_model // h\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # below are the 3 weights that the input embeddings are cloned and \n",
    "        # multiplied by to begin self-attention\n",
    "        self.wQ = nn.Linear(d_model, d_model) # Query\n",
    "        self.wK = nn.Linear(d_model, d_model) # Key\n",
    "        self.wV = nn.Linear(d_model, d_model) # Value\n",
    "        \n",
    "        # the final weight matrix \n",
    "        self.wO = nn.Linear(d_model, d_model, bias=False) # Value\n",
    "    \n",
    "    @staticmethod\n",
    "    # method is non-vectorized intentionally because it helped me understand :D\n",
    "    def attention_simple(query, key, value, mask, dk, dropout):\n",
    "        \n",
    "        # intiialize list of heads\n",
    "        heads = []\n",
    "        scores = []\n",
    "        \n",
    "        # Assume q, k, v have been reshaped to: (batch, h, seq_len, d_k)\n",
    "        # Split q, k, v into individual heads\n",
    "        q_heads = torch.unbind(query, dim=1)  # Creates a tuple of `h` tensors, each of shape (batch, seq_len, d_k)\n",
    "        k_heads = torch.unbind(key, dim=1)  # Similarly for k\n",
    "        v_heads = torch.unbind(value, dim=1)  # Similarly for v\n",
    "        \n",
    "        # for each head in Q, K, V respectively, calculate attention\n",
    "        # scores for each head using scaled dot-product and add it to list\n",
    "        for q, k, v in zip(q_heads, k_heads, v_heads):\n",
    "            \n",
    "            # calculate attention score\n",
    "            # transpose k\n",
    "            # (batch, seq, d_model) * (batch, d_model, seq) -> (batch, seq, seq)\n",
    "            attention_scores = q @ k.transpose(-2, -1)\n",
    "            \n",
    "            # attention score scaled (ASS) by dividing by sqrt(d_model/h)\n",
    "            # indicates how much attention each element should pay to other elements\n",
    "            # A[i, j] represents the importance between the i-th input \"query\" and \n",
    "            # j-th input \"key\"\n",
    "            attention_scores_scaled = attention_scores/torch.sqrt(dk)\n",
    "            \n",
    "            # Apply mask (if provided)\n",
    "            # used to ignore/pay no attention to certain positions in the sequence for padding (fillter to ensure sequences are aligned) \n",
    "            # and future tokens by setting their attention scores to -1e9\n",
    "            if mask is not None:\n",
    "                # Assuming mask has shape (batch, 1, seq_len, seq_len)\n",
    "                attention_scores = attention_scores_scaled.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "            # softmaxing the head makes it so each row sums to 1\n",
    "            attention_scores_scaled = torch.softmax(attention_scores_scaled, dim=-1)\n",
    "            \n",
    "            # adding the scaled attention vector to a seperate list\n",
    "            scores.append(attention_scores_scaled)\n",
    "            \n",
    "            # multiply attention scores by v\n",
    "            # since the head gives us attention weights, multiplying by v will\n",
    "            # propogate the attention information by weighting and adding the \n",
    "            # features of each element according to their importance to the current element\n",
    "            head_i = attention_scores_scaled @ value\n",
    "            heads.append(head_i)\n",
    "        \n",
    "        \n",
    "        # applies dropout regularization\n",
    "        if dropout is not None:\n",
    "            heads = dropout(heads)\n",
    "            \n",
    "        # returns a concatenation of the h heads\n",
    "        # Q, K, V: (batch, seq, d_model) -> (batch, h, seq, d_model/h) -> (batch, seq, d_model)\n",
    "        concatenated_heads = torch.cat(heads, dim=-1)\n",
    "        concatenated_scores = torch.cat(scores, dim=-1)\n",
    "        \n",
    "        # returns a concatenation of the heads and the scores\n",
    "        # Q, K, V: (batch, seq, d_model) -> (batch, h, seq, d_model/h) -> (batch, seq, d_model)\n",
    "        return concatenated_heads, concatenated_scores\n",
    "            \n",
    "    @staticmethod\n",
    "    # vectorized approach, more time efficient but less clear\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        h = query.shape[1]\n",
    "\n",
    "        # (Batch, h, Seq_Len, d_k) --> (Batch, h, Seq_Len, Seq_Len)\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / torch.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_scores = attention_scores.softmax(dim=-1)  # (Batch, h, seq_len, seq_len)\n",
    "        \n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            \n",
    "        # (Batch, h, Seq_Len, Seq_Len) x (Batch, h, Seq_Len, d_k) -> (Batch, h, Seq_Len, d_k)\n",
    "        x = attention_scores @ value\n",
    "        \n",
    "        # (Batch, h, Seq_Len, d_k) -> (Batch, Seq_Len, h, d_k) -> (Batch, Seq_Len, d_model)\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, h * d_k)\n",
    "        \n",
    "        return x, attention_scores\n",
    "\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        \n",
    "        # multiplied by respective weights\n",
    "        # 3 x (batch, seq_len, d_model) -> (batch, seq_len, d_model)\n",
    "        query = self.wQ(q)\n",
    "        key = self.wK(k)\n",
    "        value = self.wV(v)\n",
    "        \n",
    "        \n",
    "        # Assume q, k, v have the shape: (batch, seq, d_model)\n",
    "        batch_size = q.shape[0]\n",
    "        seq_len = q.shape[1]\n",
    "\n",
    "        # Reshape q, k, v to have separate heads and transpose to \n",
    "        # bring the heads dimension before the sequence dimension\n",
    "        # New shape: (batch, seq, h, d_model // h)\n",
    "        query = query.view(batch_size, seq_len, self.h, self.dk).transpose(1, 2)\n",
    "        key = key.view(batch_size, seq_len, self.h, self.dk).transpose(1, 2)\n",
    "        value = value.view(batch_size, seq_len, self.h, self.dk).transpose(1, 2)\n",
    "        \n",
    "        # \n",
    "        x, self.attention_scores = MultiHeadedAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        # run this through the final weight matrix in this block\n",
    "        return self.wO(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the \"Add and Norm\" block\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization()\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        \n",
    "        # original paper passes x through the sublayer before normalizing it\n",
    "        # but most implementations do it the other way around as below\n",
    "        sublayer_processed = self.dropout(sublayer(self.norm(x)))\n",
    "        \n",
    "        # raw input is combined with input that is processed\n",
    "        residual_connection = x + sublayer_processed\n",
    "        \n",
    "        return residual_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, self_attention_block, feed_forward_block, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        \n",
    "        # 1) residual connection for Attention Output = LayerNorm(Input + MultiHeadAttention(Input))\n",
    "        # 2) residual connection for FFN Output = LayerNorm(Input + FeedForwardNetwork(Input)) \n",
    "        self.residual_connection = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
    "        \n",
    "    # src_mask ensures \"padding\" words don't interact with actual words \n",
    "    def forward(self, x, src_mask):\n",
    "        \n",
    "        attention_output = self.self_attention_block(x, x, x, src_mask)\n",
    "        x = self.residual_connection[0](x, attention_output)\n",
    "\n",
    "        feed_forward_output = self.feed_forward_block(x)\n",
    "        x = self.residual_connection[1](x, feed_forward_output)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # represents the N encoder block layers\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization()\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # each successive layer consumes the previous layer recursively\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        # normalize\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
